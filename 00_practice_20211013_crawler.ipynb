{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00_practice_20211013-crawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stinh/Tiba_02_Crawler-and-SQL/blob/main/00_practice_20211013_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykYTBNwuSCtM"
      },
      "source": [
        "# Part 1：抓經濟日報即時新聞\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import prettytable as pt\n",
        "\n",
        "url = \"https://money.udn.com/rank/newest/1001/0/1\"\n",
        "head = {\n",
        "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n",
        "}\n",
        "\n",
        "resp = requests.get(url, headers=head)\n",
        "\n",
        "# 確認resp有拿到東西，就可以把resp.text丟到BeautifulSoup裡面去解析\n",
        "source = BeautifulSoup(resp.text, \"html.parser\")\n",
        "news_rows = source.find_all(\"tr\", {\"style\": \"table-row\"})\n",
        "\n",
        "# 確認底下forloop裡面資料都可以正常抓到以後，來建立一個prettytable存放資料\n",
        "table = pt.PrettyTable([\"標題\", \"類別\", \"時間\", \"網址\"])\n",
        "# 微調一下對齊方式\n",
        "table.align[\"標題\"] = \"l\"\n",
        "\n",
        "# 剛剛抓到的news_rows是一個list，用for loop針對內容做進一步搜尋\n",
        "for news in news_rows:\n",
        "    # 抓出新聞的標題文字，找a標籤\n",
        "    title = news.find(\"a\").text\n",
        "    # 抓出新聞類別所在的td標籤，需要詳細的屬性(attrs)\n",
        "    category = news.find(\"td\", {\"class\": \"only_web\"}).text\n",
        "    # 抓出新聞發布時間\n",
        "    time = news.find(\"td\", {\"align\": \"right\", \"class\": \"only_1280\"}).text\n",
        "    # 抓出新聞網址連結\n",
        "    link = news.find(\"a\").attrs[\"href\"]\n",
        "    # 把資料放進做好的table中\n",
        "    table.add_row([title, category, time, link])\n",
        "\n",
        "# 最後印出整個做好的table\n",
        "print(table)\n",
        "\n",
        "\n",
        "# Part 2: 將以上資料點進每個新聞連接後抓取新聞內文，並存檔出來\n",
        "import codecs\n",
        "\n",
        "# 這邊我想稍微處理一下存檔檔名當作每篇文章的標題，因為時間有標點符號，會用replace()方法稍做處理\n",
        "for i, news in enumerate(news_rows):\n",
        "    # 抓出新聞的標題文字，找a標籤\n",
        "    title = news.find(\"a\").text\n",
        "    # 抓出新聞網址連結\n",
        "    link = news.find(\"a\").attrs[\"href\"]\n",
        "    # 抓出時間\n",
        "    time = news.find(\"td\", {\"align\": \"right\", \"class\": \"only_1280\"}).text\n",
        "    time_mdf = time.replace(\"/\", \"_\").replace(\" \", \"_\").replace(\":\", \"\")\n",
        "    # 抓出網址內的內文，先送requests.get，再去抓單篇文章的內容\n",
        "    article_resp = requests.get(link, headers=head)\n",
        "    soup = BeautifulSoup(article_resp.text, \"html.parser\")\n",
        "\n",
        "    article_p_list = soup.find(\"div\", {\"id\": \"article_body\"}).find_all(\"p\")\n",
        "\n",
        "    with codecs.open(f\"1013_news/{time_mdf}-{title}.txt\", \"w\", encoding=\"utf-8\") as save_file:\n",
        "        # 開啟檔案後寫入存在article_p_list裡面抓到的文章段落\n",
        "        for article_p in article_p_list:\n",
        "            save_file.write(article_p.text)\n",
        "        # 在文末加入斷行與文章連結    \n",
        "        save_file.write(\"\\n\")\n",
        "        save_file.write(link)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}